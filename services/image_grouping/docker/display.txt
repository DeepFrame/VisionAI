#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\.env ******************************************

SQL_CONNECTION_STRING="Driver={ODBC Driver 17 for SQL Server};Server={(localdb)\MSSQLLocalDB};Database=FaceRecognitionSystem;Encrypt=no;TrustServerCertificate=no;"

THUMBNAIL_SAVE_PATH="\app\services\image_grouping\image_face_detection\Thumbnails"

IMAGE_SAVE_PATH="\app\services\image_grouping\image_face_detection\Images"


#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\config.py ******************************************

import os
from dotenv import load_dotenv

load_dotenv()

SQL_CONNECTION_STRING = os.getenv("SQL_CONNECTION_STRING")

THUMBNAIL_SAVE_PATH = os.getenv("THUMBNAIL_SAVE_PATH", "./Thumbnails")

os.makedirs(THUMBNAIL_SAVE_PATH, exist_ok=True)

os.environ["CUDA_VISIBLE_DEVICES"] = ""



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\main.py ******************************************

import argparse
import time
import pyodbc

from image_face_detection.detect_faces import (
    test_single_image,
    batch_process_from_db,
    continuous_batch_process,
    countdown_timer
)
from person_recognition.recognize_persons import main as recognize_persons_main

from config import SQL_CONNECTION_STRING

def should_recluster():
    """Check if there are entries in the Persons table."""
    try:
        conn = pyodbc.connect(SQL_CONNECTION_STRING)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM dbo.Persons")
        count = cursor.fetchone()[0]
        cursor.close()
        conn.close()
        return count > 0 
    except Exception as e:
        print(f"[ERROR] Failed to check Persons table: {e}")
        return False

def full_pipeline_once(recluster=False, dry_run=False):
    """Run detection + recognition once."""
    print("[01] Detecting faces...")
    batch_process_from_db(dry_run=dry_run)
    print("[02] Running recognition pipeline...")
    recognize_persons_main(recluster=recluster, dry_run=dry_run)
    print("[INFO] Full pipeline completed.")

def automated_pipeline(interval_minutes=3, recluster=False, dry_run=False):
    """Run full pipeline every N minutes."""
    print(f"[AUTO] Pipeline execution started...")
    while True:
        start_time = time.strftime("%Y-%m-%d %H:%M:%S")
        print(f"\n[{start_time}] Running full pipeline...")
        full_pipeline_once(recluster=recluster, dry_run=dry_run)
        wait_seconds = interval_minutes * 60
        print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Sleeping for {interval_minutes} minutes...")
        countdown_timer(wait_seconds, message="Next run starts in")

def main():
    parser = argparse.ArgumentParser(description="Face Recognition System CLI")
    parser.add_argument("--test", type=str, help="Run face detection on single image")
    parser.add_argument("--db", action="store_true", help="Run face detection on DB media items once")
    parser.add_argument("--watch", action="store_true", help="Run continuous monitoring and processing (detection)")
    parser.add_argument("--recognize", action="store_true", help="Run embeddings + clustering pipeline")
    parser.add_argument("--all", action="store_true", help="Run full pipeline: detection -> recognition once")
    parser.add_argument("--automate", action="store_true", help="Run full pipeline every 3 minutes: detection + recognition")
    parser.add_argument("--recluster", action="store_true", help="Rebuild clusters for all faces (ignore old PersonId mappings)")
    parser.add_argument("--dry-run", action="store_true", help="Produces logs without DB changes")

    args = parser.parse_args()

    if args.test:
        test_single_image(args.test, dry_run=args.dry_run)
    elif args.db:
        batch_process_from_db()
    elif args.watch:
        continuous_batch_process(dry_run=args.dry_run)
    elif args.recognize:
        recognize_persons_main(recluster=args.recluster)
    elif args.all:
        full_pipeline_once(recluster=args.recluster)
    elif args.automate:
        recluster_needed = should_recluster()
        print(f"Persons table entries found: {recluster_needed}. Setting recluster={recluster_needed}")
        automated_pipeline(interval_minutes=3, recluster=recluster_needed, dry_run=args.dry_run)
    elif args.dry_run:
        print("Dry run mode activated. No DB changes will be made.")
        recluster_needed = should_recluster()
        full_pipeline_once(recluster=recluster_needed, dry_run=True)
    elif args.recluster:
        print("Recluster flag set. This will affect the next recognition run.")
        recluster_needed = should_recluster()
        print(f"Persons table entries found: {recluster_needed}. Setting recluster={recluster_needed}")
        automated_pipeline(interval_minutes=3, recluster=recluster_needed, dry_run=args.dry_run)
    else:
        print("Use --test <image_path>, --db, --watch, --recognize, --all, --automate, --dry-run, or --recluster")

if __name__ == "__main__":
    main()



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\person_recognition\logger_config.py ******************************************

import os
import logging

BASE_DIR = os.path.dirname(__file__)
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)

LOG_PATH = os.path.join(LOG_DIR, "embeddings_clustering.log")

def get_logger(name="person_recognition"):
    logger = logging.getLogger(name)
    if not logger.handlers:  # prevent duplicate handlers
        handler = logging.FileHandler(LOG_PATH)
        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\person_recognition\recognize_persons.py ******************************************

import os
import cv2
import json
import logging
import pyodbc
import numpy as np
from datetime import datetime
from keras_facenet import FaceNet
from sklearn.cluster import DBSCAN
import umap
from tabulate import tabulate

from collections import defaultdict

from sklearn.metrics.pairwise import cosine_similarity
import shutil

# logger setup
from .logger_config import get_logger
logger = get_logger()
    
# DB connection
from config import SQL_CONNECTION_STRING

conn_str = SQL_CONNECTION_STRING

# FaceNet Embedder
embedder = FaceNet()

# DB Utilities
def get_faces_with_bboxes():
    """Fetch faces missing embeddings (NULL Embedding)."""
    conn = pyodbc.connect(conn_str)
    cursor = conn.cursor()
    cursor.execute("""
        SELECT 
            F.Id, MF.FilePath, MF.FileName, F.BoundingBox
        FROM dbo.Faces F
        JOIN dbo.MediaItems MI ON F.MediaItemId = MI.Id
        JOIN dbo.MediaFile MF ON MI.MediaFileId = MF.Id
        WHERE F.Embedding IS NULL
    """)
    rows = cursor.fetchall()
    cursor.close()
    conn.close()

    results = []
    for row in rows:
        face_id, file_path, file_name, bbox_json = row
        try:
            bbox = json.loads(bbox_json) if bbox_json else None
        except:
            bbox = None
        full_path = file_path+"/"+file_name
        logger.info(f"[DB] FaceId={face_id}, File={full_path}, BBox={bbox}")
        results.append({
            "FaceId": face_id,
            "FullPath": full_path,
            "BoundingBox": bbox
        })
    return results

def update_face_embedding(face_id, embedding):
    """Update DB with generated embedding."""
    try:
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()
        emb_bytes = embedding.astype(np.float32).tobytes()
        cursor.execute("""
            UPDATE dbo.Faces
            SET Embedding = ?, ModifiedAt = ?
            WHERE Id = ?
        """, emb_bytes, datetime.now(), face_id)
        conn.commit()
        cursor.close()
        conn.close()
        logger.info(f"Updated embedding for FaceId={face_id}")
    except Exception as e:
        logger.error(f"Failed to update embedding for FaceId {face_id}: {e}")

# Compute medoid of a cluster - PORTRAIT GENERATION
def compute_cluster_medoid(face_files, embeddings, cluster_label, labels):
    """
    Return the path of the medoid face for a given cluster.
    Medoid = face whose embedding is closest on average to all others in the cluster.
    """
    cluster_indices = [i for i, lbl in enumerate(labels) if lbl == cluster_label]
    cluster_embeddings = embeddings[cluster_indices]
    cluster_faces = [face_files[i] for i in cluster_indices]

    sim_matrix = cosine_similarity(cluster_embeddings)
    dist_matrix = 1 - sim_matrix 
    total_distances = dist_matrix.sum(axis=1)
    medoid_idx = np.argmin(total_distances)
    
    return cluster_faces[medoid_idx]

def calculate_sharpness(image_path):
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if image is None:
        return 0 
    laplacian_var = cv2.Laplacian(image, cv2.CV_64F).var()
    return laplacian_var

def generate_portraits(rows, portrait_dir, dry_run=False):
    if not os.path.exists(portrait_dir) and not dry_run:
        os.makedirs(portrait_dir)

    faces_by_person = defaultdict(list)

    for row in rows:
        person_id = row.PersonId 
        faces_by_person[person_id].append({
            "FaceId": row.FaceId,
            "FaceImagePath": row.FaceImagePath,
            "Embedding": parse_embedding(row.Embedding)
        })

    #logger.info(f"\n\nFaces grouped by person: {dict(faces_by_person)}")
    logger.info(f"\n\nTotal persons to process: {len(faces_by_person)}")
    logger.info(f"faces_by_person keys: {list(faces_by_person.keys())}")

    face_files = [row.FaceImagePath for row in rows]
    face_embeddings = np.array([parse_embedding(row.Embedding) for row in rows])
    labels = [row.PersonId for row in rows]

    logger.info(f"face_files: {face_files} \nface_embeddings shape: {face_embeddings.shape} \nlabels: {labels}")

    for person_id, face_entries in faces_by_person.items():
        cluster_embeddings = np.array([f["Embedding"] for f in face_entries])
        cluster_faces = [f["FaceImagePath"] for f in face_entries]
        cluster_face_ids = [f["FaceId"] for f in face_entries]

        logger.info(f"Processing PersonId={person_id} with {len(cluster_faces)} faces.")

        sim_matrix = cosine_similarity(cluster_embeddings)
        dist_matrix = 1 - sim_matrix
        total_distances = dist_matrix.sum(axis=1)
        medoid_idx = np.argmin(total_distances)

        medoid_path = cluster_faces[medoid_idx]
        medoid_face_id = cluster_face_ids[medoid_idx]

        medoid_sharpness = calculate_sharpness(medoid_path)
        for i, path in enumerate(cluster_faces):
            candidate_sharpness = calculate_sharpness(path)
            if candidate_sharpness > medoid_sharpness:
                medoid_path = path
                medoid_face_id = cluster_face_ids[i]
                medoid_sharpness = candidate_sharpness

        logger.info(f"\nSelected medoid for Person {person_id}: {medoid_path} (FaceId={medoid_face_id}, Sharpness={medoid_sharpness})")

        if not dry_run and medoid_path:
            output_path = os.path.join(portrait_dir, f"Person_{person_id}_portrait.jpg")
            shutil.copy(medoid_path, output_path)
            logger.info(f"Saved portrait for Person {person_id}: {output_path}\n\n")

            update_portrait_face_id(person_id, medoid_face_id)

def update_portrait_face_id(person_id, face_id):
    """Update Persons.PortraitFaceId with the chosen FaceId"""
    try:
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE dbo.Persons
            SET PortraitFaceId = ?
            WHERE Id = ?;
        """, (face_id, person_id))
        conn.commit()
        cursor.close()
        conn.close()
        print(f"[DB] Updated PersonId={person_id} with PortraitFaceId={face_id}")
    except Exception as e:
        print(f"[ERROR] Failed to update PortraitFaceId for PersonId={person_id}: {e}")



# Fetch faces to cluster
def get_unassigned_faces(recluster=False, labelled=False):
    conn = pyodbc.connect(conn_str)
    cursor = conn.cursor()

    if recluster:
        cursor.execute("""
            SELECT Id, Embedding
            FROM dbo.Faces
            WHERE Embedding IS NOT NULL
        """)
    elif labelled:
        cursor.execute("""
            SELECT Id, Embedding, PersonId
            FROM dbo.Faces
            WHERE PersonId IS NOT NULL AND Embedding IS NOT NULL
        """)
    else:
        cursor.execute("""
            SELECT Id, Embedding
            FROM dbo.Faces
            WHERE PersonId IS NULL AND Embedding IS NOT NULL
        """)

    rows = cursor.fetchall()
    cursor.close()
    conn.close()
    return rows

def get_faces_with_paths(conn_str=conn_str):
    conn = pyodbc.connect(conn_str)
    cursor = conn.cursor()

    cursor.execute("""
        SELECT 
            p.Id AS PersonId,
            f.Id AS FaceId,
            f.Name AS FaceName,
            'C:\\Users\\ADMIN\\Downloads\\FRS_Project\\services\\image_grouping\\image_face_detection\\Thumbnails\\'
                + f.Name AS FaceImagePath,
            f.Embedding
        FROM dbo.Faces f
        INNER JOIN dbo.Persons p 
            ON f.PersonId = p.Id
        ORDER BY p.Id, f.Id;
    """)

    rows = cursor.fetchall()
    columns = [column[0] for column in cursor.description]

    cursor.close()
    conn.close()
    return rows, columns

def parse_embedding(raw_value):
    try:
        if isinstance(raw_value, bytes):
            return np.frombuffer(raw_value, dtype=np.float32)
        elif isinstance(raw_value, str):
            return np.array(json.loads(raw_value), dtype=np.float32)
        return None
    except Exception as e:
        logger.error(f"Embedding parse error: {e}")
        return None

# Update Database with clusters
def assign_clusters(labels, face_ids, recluster=False, existing_person_id=None, dry_run=False):
    """
    Assign clusters to faces.
    - If existing_person_id is provided, assign faces directly to that person.
    - Otherwise, create new person entries per cluster.
    """
    try:
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()

        # === Direct assignment to existing person ===
        if existing_person_id is not None:
            if dry_run:
                logger.info(f"Dry run: Assigning {len(face_ids)} faces to PersonId={existing_person_id}")
            else:
                logger.info(f"Assigning {len(face_ids)} faces to existing PersonId={existing_person_id}")
                for face_id in face_ids:
                    cursor.execute("EXEC dbo.UpsertFace @FaceId=?, @PersonId=?", (face_id, existing_person_id))
                    cursor.execute("EXEC dbo.UpsertPerson ?, ?, ?, ?", (int(existing_person_id), None, None, None))

            conn.commit()
            cursor.close()
            conn.close()
            return

        # === Normal clustering mode ===
        logger.info(f"Assigning {len(set(labels))} clusters to {len(face_ids)} faces...")
        for cluster_id in set(labels):
            if cluster_id == -1: 
                continue
            if not dry_run:
                cursor.execute("""
                    INSERT INTO dbo.Persons (Name, Rank, Appointment, CreatedAt)
                    OUTPUT INSERTED.Id
                    VALUES (?, ?, ?, ?)
                """, f"Unknown-{int(cluster_id)}", None, None, datetime.now())
            person_id = int(cursor.fetchone()[0])

            logger.info(f"Created new PersonId={person_id} for cluster {cluster_id}")

            for face_id, label in zip(face_ids, labels):
                if label == cluster_id and not dry_run:
                    cursor.execute("""
                        UPDATE dbo.Faces
                        SET PersonId = ?, ModifiedAt = ?
                        WHERE Id = ?
                    """, person_id, datetime.now(), int(face_id))

            if not dry_run:
                cursor.execute("""
                    UPDATE dbo.Persons
                    SET ModifiedAt = ?
                    WHERE Id = ?
                """, datetime.now(), person_id)

        conn.commit()
        cursor.close()
        conn.close()

    except Exception as e:
        logger.error(f"DB update failed (assign clusters): {e}")

# Processing Pipeline
def process_missing_embeddings(dry_run=False):
    """Find faces with NULL embedding, generate, and update."""
    logger.info("Starting embedding process...")

    rows = get_faces_with_bboxes()
    if not rows:
        print("[INFO] No missing embeddings found.")
        return

    for row in rows:
        face_id, full_path, bbox = row["FaceId"], row["FullPath"], row["BoundingBox"]

        if not os.path.exists(full_path):
            logger.warning(f"File not found: {full_path}")
            continue

        img = cv2.imread(full_path)
        if img is None or bbox is None:
            logger.warning(f"Invalid image/bbox for FaceId={face_id}")
            continue

        try:
            x1, y1, x2, y2 = [int(v) for v in bbox]
            face_crop = img[y1:y2, x1:x2]
            if face_crop.size == 0:
                logger.warning(f"Empty crop for FaceId={face_id}")
                continue

            face_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
            face_crop = cv2.resize(face_crop, (160, 160))

            emb = embedder.embeddings([face_crop])[0]
            if dry_run:
                logger.info(f"[Dry Run] Generated embedding for FaceId={face_id} (not updating DB)")
            else:
                update_face_embedding(face_id, emb)

        except Exception as e:
            logger.error(f"Embedding failed for FaceId={face_id}: {e}")

def recluster_unlabelled_faces(eps=0.35, min_samples=3, similarity_threshold=0.75, dry_run=False):
    labelled_rows = get_unassigned_faces(labelled=True) 
    labelled_ids, labelled_embeddings, labelled_persons = [], [], []
    for row in labelled_rows:
        face_id, raw_emb, person_id = row 
        emb = parse_embedding(raw_emb)
        if emb is not None:
            labelled_ids.append(face_id)
            labelled_embeddings.append(emb)
            labelled_persons.append(person_id)

    labelled_embeddings = np.array(labelled_embeddings)

    logger.info(f"Found {len(labelled_ids)} labelled faces for reclustering.")
    if labelled_embeddings.size == 0:
        print("[INFO] No labelled embeddings found for reclustering.")
        return
    
    unlabelled_rows = get_unassigned_faces(recluster=False)
    unlabelled_ids, unlabelled_embeddings = [], []
    for row in unlabelled_rows:
        face_id, raw_emb = row
        emb = parse_embedding(raw_emb)
        if emb is not None:
            unlabelled_ids.append(face_id)
            unlabelled_embeddings.append(emb)

    logger.info(f"Found {len(unlabelled_ids)} unlabelled faces for reclustering.")
    if not unlabelled_embeddings:
        print("[INFO] No unlabelled embeddings to recluster.")
        return

    unlabelled_embeddings = np.array(unlabelled_embeddings)
    logger.info(f"Unlabelled embeddings shape: {unlabelled_embeddings.shape}")
    logger.info(f"Unlabelled embeddings ids: " + ", ".join(map(str, unlabelled_ids)))
    for idx, emb in enumerate(unlabelled_embeddings):
        if labelled_embeddings.size == 0:
            break
        sims = cosine_similarity([emb], labelled_embeddings)[0]
        
        max_idx = np.argmax(sims)
        
        logger.info(f"FaceId={unlabelled_ids[idx]} similarity with PersonId={labelled_persons[max_idx]}: {sims[max_idx]:.4f}")
        if sims[max_idx] >= similarity_threshold:
            logger.info(f"Assigning FaceId={unlabelled_ids[idx]} to PersonId={labelled_persons[max_idx]}")
            assign_clusters(labels=None, face_ids=[unlabelled_ids[idx]], existing_person_id=labelled_persons[max_idx], dry_run=dry_run)

            labelled_ids.append(unlabelled_ids[idx])
        else:
            logger.info(f"FaceId={unlabelled_ids[idx]} does not meet similarity threshold, skipping.")
            continue

    remaining_ids = [fid for fid in unlabelled_ids if fid not in labelled_ids]
    remaining_embeddings = np.array([emb for i, emb in enumerate(unlabelled_embeddings) if unlabelled_ids[i] in remaining_ids])
    
    logger.info(f"Remaining unlabelled faces to cluster: {len(remaining_ids)}")
    
    if remaining_embeddings.size > 0:
        logger.info("Running DBSCAN on remaining unlabelled faces...")
        clustering = DBSCAN(eps=eps, min_samples=min_samples, metric="cosine")
        labels = clustering.fit_predict(remaining_embeddings)
        logger.info(f'Labels found: {set(labels)}')
        if len(set(labels)) <= 1:
            logger.info("No significant clusters found in remaining unlabelled faces.")
            return
        assign_clusters(labels, remaining_ids, recluster=False, dry_run=dry_run)
        logger.info(f"Reclustering complete. Assigned {len(set(labels))} clusters to remaining unlabelled faces.")

# Recognition pipeline: embeddings + clustering.
def main(recluster=False, dry_run=False):
    print("[Step 1:] Generating embeddings for missing faces...")
    process_missing_embeddings(dry_run=dry_run)

    if recluster:
        print("[Step 2:] Reclustering unlabelled faces...")
        recluster_unlabelled_faces(dry_run=dry_run)
    else:
        print("[Step 2:] Clustering unlabelled faces...")
        rows = get_unassigned_faces(recluster=False)
        if not rows:
            print("[INFO] No faces available for clustering.")
            return

        face_ids, embeddings = [], []
        for row in rows:
            face_id, raw_emb = row
            emb = parse_embedding(raw_emb)
            if emb is not None:
                face_ids.append(face_id)
                embeddings.append(emb)

        embeddings = np.array(embeddings)
        if embeddings.size == 0:
            print("[WARN] No valid embeddings found.")
            return

        # Dimensionality reduction
        reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric="cosine")
        embeddings_2d = reducer.fit_transform(embeddings)

        clustering = DBSCAN(eps=0.35, min_samples=3, metric="cosine")
        labels = clustering.fit_predict(embeddings)

        num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        logger.info(f"Found {num_clusters} clusters")

        assign_clusters(labels, face_ids, recluster=False, dry_run=dry_run)

        print(f"[INFO] Finished. Found {num_clusters} clusters.")
    
    portrait_dir = "individuals_portraits"

    if not os.path.exists(portrait_dir):
        os.makedirs(portrait_dir)

    rows, columns = get_faces_with_paths()
    logger.info(f"Current DB entries: {len(rows)} faces, columns: {columns}")
    logger.info(tabulate(rows, headers=columns, tablefmt="psql"))

    generate_portraits(rows, portrait_dir, dry_run=dry_run)
    print("[INFO] Portraits saved to 'individuals_portraits/' directory.")
    logger.info("Portrait generation complete.")



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\image_face_detection\logger_config.py ******************************************

import os
import logging

BASE_DIR = os.path.dirname(__file__)
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)

LOG_PATH = os.path.join(LOG_DIR, "face_detection.log")

def get_logger(name="image_face_detection"):
    logger = logging.getLogger(name)
    if not logger.handlers:
        handler = logging.FileHandler(LOG_PATH)
        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\image_face_detection\detect_faces.py ******************************************

import os
import time
import pyodbc
import cv2
import numpy as np
from datetime import datetime
from retinaface import RetinaFace
from tabulate import tabulate
#from .config import SQL_CONNECTION_STRING, THUMBNAIL_SAVE_PATH
from config import SQL_CONNECTION_STRING, THUMBNAIL_SAVE_PATH

import sys
import time

import json

import logging

# logger setup
from .logger_config import get_logger
logger = get_logger()

# Load environment variables
conn_str = SQL_CONNECTION_STRING
thumbnail_base_path = THUMBNAIL_SAVE_PATH

# Ensure the thumbnail directory exists
os.makedirs(thumbnail_base_path, exist_ok=True)

# DATABASE Unprocessed files and Thumbnails Query Processing
def get_unprocessed_files():
    try:
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()

        query = """
        SELECT MI.Id, MF.FilePath, MI.FileName
        FROM dbo.MediaItems MI
        JOIN dbo.MediaFile MF ON MI.MediaFileId = MF.Id
        WHERE MI.IsFacesExtracted = 0
        """
        cursor.execute(query)
        rows = cursor.fetchall()
        cursor.close()
        conn.close()

        return rows
    
    except Exception as e:
        logger.error(f"Database update failed: {e}")
        return []

def get_next_thumbnail_id(cursor):
    cursor.execute("""
        SELECT TOP 1 Id 
        FROM dbo.ThumbnailStorage 
        WHERE Id LIKE 'TS%' 
        ORDER BY Id DESC
    """)
    row = cursor.fetchone()
    if row:
        last_id = row[0]  
        last_num = int(last_id[2:]) 
        next_num = last_num + 1
    else:
        next_num = 1

    return f"TS{next_num:03d}" 

def get_unassigned_faces():
    try:
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()
        cursor.execute("""
            SELECT Id, Embedding
            FROM dbo.Faces
            WHERE PersonId IS NULL
        """)
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        return rows
    except Exception as e:
        logger.error(f"Could not load unassigned faces: {e}")
        #print(f"[ERROR] Could not load unassigned faces: {e}")
        return []
    
def parse_embedding(raw_value):
    try:
        if isinstance(raw_value, str):
            return np.array(json.loads(raw_value), dtype=np.float32)
        # If stored as varbinary
        elif isinstance(raw_value, bytes):
            return np.frombuffer(raw_value, dtype=np.float32)
        else:
            raise ValueError("Unknown embedding format")
    except Exception as e:
        logger.error(f"Failed to parse embedding: {e}")
        return None

# Filter Non-Blurry Images
def is_blurry(image, threshold=100.0):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    variance = cv2.Laplacian(gray, cv2.CV_64F).var()
    return variance < threshold, variance

# DATABASE Update Query Processing
def update_database(media_item_id, face_bboxes, filename=None):
    """
    Inserts detected faces into dbo.Faces (BoundingBox only, no embedding yet).
    Avoids duplicates for same MediaItemId + BoundingBox.
    Updates ModifiedAt if duplicate exists.
    """
    try:
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()

        # Update MediaItems status
        cursor.execute("""
            UPDATE dbo.MediaItems
            SET IsFacesExtracted = 1,
                FacesExtractedOn = ?
            WHERE Id = ?
        """, (datetime.now(), media_item_id))

        inserted_count = 0
        updated_count = 0

        for bbox in face_bboxes:
            # Ensure bbox is in the correct format [x1, y1, x2, y2]
            if not isinstance(bbox, (list, tuple)) or len(bbox) != 4:
                logger.warning(f"Invalid bounding box format: {bbox}")
                continue

            try:
                bbox_norm = [round(float(c), 3) for c in bbox] 
                bbox_str = json.dumps(bbox_norm) 

                cursor.execute("""
                    SELECT Id FROM dbo.Faces
                    WHERE MediaItemId = ? AND BoundingBox = ?
                """, (media_item_id, bbox_str))
                row = cursor.fetchone()

                if row:
                    cursor.execute("""
                        UPDATE dbo.Faces
                        SET BoundingBox = ?, Name = ?, ModifiedAt = ?
                        WHERE Id = ?
                    """, (bbox_str, filename, datetime.now(), row[0]))
                    updated_count += 1
                else:
                    cursor.execute("""
                        INSERT INTO dbo.Faces (MediaItemId, BoundingBox, Name, CreatedAt)
                        VALUES (?, ?, ?, ?)
                    """, media_item_id, bbox_str, filename, datetime.now())
                    inserted_count += 1
            except Exception as e:
                logger.warning(f"Failed to process bounding box {bbox}: {e}")
                continue

        conn.commit()
        cursor.close()
        conn.close()

        logger.info(f"Inserted {inserted_count} new face(s), updated {updated_count} existing face(s) for MediaItemId {media_item_id}")

    except Exception as e:
        logger.error(f"Database update failed: {e}")
        if 'conn' in locals():
            conn.rollback()

# Countdown Timer for Continuous Processing
def countdown_timer(seconds, message="Waiting"):
    for remaining in range(seconds, 0, -1):
        mins, secs = divmod(remaining, 60)
        sys.stdout.write(f"\r{message}: {mins:02d}:{secs:02d} remaining ")
        sys.stdout.flush()
        time.sleep(1)
    sys.stdout.write("\r" + " " * 50 + "\r")

# SQUARE CROP with margin & expansion/shrink
def process_face_square(img, face, margin_ratio=0.2, target_size=(112, 112)):
    #score = face.get("score", 0.0)
    
    h, w = img.shape[:2]
    x1, y1, x2, y2 = face["facial_area"]

    bw = x2 - x1
    bh = y2 - y1
    margin_x = int(bw * margin_ratio)
    margin_y = int(bh * margin_ratio)

    x1 = max(0, x1 - margin_x)
    y1 = max(0, y1 - margin_y)
    x2 = min(w, x2 + margin_x)
    y2 = min(h, y2 + margin_y)

    crop_w = x2 - x1
    crop_h = y2 - y1
    if crop_w > crop_h:
        diff = crop_w - crop_h
        expand_top = diff // 2
        expand_bottom = diff - expand_top
        if y1 - expand_top >= 0 and y2 + expand_bottom <= h:
            y1 -= expand_top
            y2 += expand_bottom
        else:
            x1 += diff // 2
            x2 -= (diff - diff // 2)
    elif crop_h > crop_w:
        diff = crop_h - crop_w
        expand_left = diff // 2
        expand_right = diff - expand_left
        if x1 - expand_left >= 0 and x2 + expand_right <= w:
            x1 -= expand_left
            x2 += expand_right
        else:
            y1 += diff // 2
            y2 -= (diff - diff // 2)

    x1, x2 = max(0, x1), min(w, x2)
    y1, y2 = max(0, y1), min(h, y2)

    cropped_face = img[y1:y2, x1:x2]

    if cropped_face.size == 0:
        return None, None

    resized_face = cv2.resize(cropped_face, target_size, interpolation=cv2.INTER_AREA)
    updated_bbox = [int(x1), int(y1), int(x2), int(y2)]  
    return resized_face, updated_bbox

# FACE DETECTION (RetinaFace)
def detect_and_crop_faces(image_path, media_item_id=None, dry_run=False):
    try:
        img = cv2.imread(image_path)
        if img is None:
            logger.error(f"Could not read image: {image_path}")
            return False

        faces = RetinaFace.detect_faces(image_path)
        if not isinstance(faces, dict) or len(faces) == 0:
            logger.info(f"No faces detected in {image_path}")
            return False

        base_name = os.path.basename(image_path)
        name_without_ext, ext = os.path.splitext(base_name)
        ext = ext.lower() if ext else ".jpg"

        bounding_boxes = []
        
        for idx, (key, face_data) in enumerate(faces.items()):
            processed_face, bbox = process_face_square(img, face_data, margin_ratio=0.2, target_size=(112, 112))
            if processed_face is None or bbox is None:
                logger.warning(f"Face not found.")
                continue

            filename = f"{name_without_ext}_TN{idx + 1}{ext}"
            save_path = os.path.join(thumbnail_base_path, filename)
            
            try:
                if not dry_run:
                    cv2.imwrite(save_path, processed_face)
                    logger.info(f"Saved square (112x112) face {idx+1} to {save_path}")
                    # Immediately update DB per face
                    update_database(media_item_id, [bbox], filename)
                else:
                    logger.info(f"[Dry Run] Would save face {idx+1} to {save_path}")
            except Exception as e:
                logger.error(f"Failed to save face {idx+1}: {e}")

        if media_item_id is not None and bounding_boxes and not dry_run:
            update_database(media_item_id, bounding_boxes, filename)

        return True

    except Exception as e:
        logger.error(f"Face processing failed for {image_path}: {e}")
        return False
    
# MAIN BATCH PROCESSOR
def batch_process_from_db(dry_run=False):
    logger.info("Starting batch face detection from DB...")

    rows = get_unprocessed_files()
    if not rows:
        logger.info("No unprocessed files found.")
        return

    print(tabulate(rows, headers=["MediaItemId", "FilePath", "FileName"], tablefmt="grid"))
    logger.info(tabulate(rows, headers=["MediaItemId", "FilePath", "FileName"], tablefmt="grid"))
    logger.info(f"Found {len(rows)} unprocessed files.")

    for row in rows:
        media_item_id, file_path, file_name = row
        full_path = os.path.join(file_path, file_name)

        logger.info(f"\nProcessing MediaItemId {media_item_id}: {full_path}")
        detect_and_crop_faces(full_path, media_item_id=media_item_id, dry_run=dry_run)

    print("[INFO] Batch processing complete.")
    logger.info("Batch processing complete.")

# TEST SINGLE IMAGE
def test_single_image(image_path, dry_run=False):
    print(f"[INFO] Testing face detection on single image: {image_path}")
    test_detect_and_crop_faces(image_path, dry_run=dry_run)

# CONTINUOUS BATCH PROCESSING
def continuous_batch_process(dry_run=False):
    print("[INFO] Starting continuous face detection monitoring...")
    no_data_attempts = 0

    while True:
        rows = get_unprocessed_files()

        if not rows:
            no_data_attempts += 1
            print(f"[INFO] No unprocessed files found. Attempt {no_data_attempts}/5")
            if no_data_attempts >= 5:
                print("[INFO] No data after 5 attempts. Pausing for 4 minutes before next check...")
                #time.sleep(240)
                countdown_timer(4 * 60, "Next check in")
                no_data_attempts = 0
            else:
                #time.sleep(10) 
                countdown_timer(10, "Next check in")
            continue
        else:
            no_data_attempts = 0 

        #print(tabulate(rows, headers=["MediaItemId", "FilePath", "FileName"], tablefmt="grid"))

        logger.info(tabulate(rows, headers=["MediaItemId", "FilePath", "FileName"], tablefmt="grid"))
        logger.info(f"Found {len(rows)} unprocessed files.")

        for row in rows:
            media_item_id, file_path, file_name = row
            full_path = os.path.join(file_path, file_name)

            logger.info(f"\nProcessing MediaItemId {media_item_id}: {full_path}")
            success = detect_and_crop_faces(full_path, media_item_id=media_item_id, dry_run=dry_run)
            if not success:
                logger.warning(f"Skipped MediaItemId {media_item_id} due to processing error.")

        print("[INFO] Completed current batch. Checking again in 10 seconds...\n")
        time.sleep(10)

# Test Detection and Cropping (Single file)
def test_detect_and_crop_faces(image_path, media_item_id=None, dry_run=False):
    try:
        img = cv2.imread(image_path)
        if img is None:
            print(f"[ERROR] Could not read image: {image_path}")
            return False

        faces = RetinaFace.detect_faces(image_path)
        if not isinstance(faces, dict) or len(faces) == 0:
            print(f"[INFO] No faces detected in {image_path}")
            return False

        for idx, (key, face_data) in enumerate(faces.items()):
            processed_face, boundings = process_face_square(img, face_data, margin_ratio=0.2, target_size=(112, 112))
            if processed_face is None:
                print(f"[WARN] Face not found.")
                continue

            filename = f"thumb_{media_item_id or 'manual'}_{idx+1}_{int(time.time())}.jpg"
            save_path = os.path.join(thumbnail_base_path, filename)
            cv2.imwrite(save_path, processed_face)
            logger.info(f"[INFO] Saved face {idx+1} to {filename}")

            if media_item_id is not None and boundings is not None and not dry_run:
                update_database(media_item_id, boundings, filename)

        print(f"[INFO] Successfully processed {len(faces)} faces in {image_path}")
        return True

    except Exception as e:
        print(f"[ERROR] Face processing failed for {image_path}: {e}")
        return False



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\docker-compose.yml ******************************************

services:
  image_grouping:
    build:
      context: .
      dockerfile: docker/Dockerfile
    
    container_name: image-grouping-service
    
    volumes:
      - ./person_recognition/logs:/app/services/image_grouping/person_recognition/logs
      - ./image_face_detection/logs:/app/services/image_grouping/image_face_detection/logs
      - ./Thumbnails:/app/services/image_grouping/Thumbnails 

    env_file:
      - docker/.env.example
      - ./.env 

    healthcheck:
      test: ["CMD-SHELL", "python ./docker/healthcheck.py"]
      interval: 10s
      timeout: 5s
      retries: 3

    restart: unless-stopped
    entrypoint: ["docker-entrypoint.sh"]



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\docker\Dockerfile ******************************************

FROM python:3.8.20

# Add these lines to your existing Dockerfile
RUN apt-get update && \
    apt-get install -y curl gnupg && \
    curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - && \
    curl https://packages.microsoft.com/config/ubuntu/20.04/prod.list > /etc/apt/sources.list.d/mssql-release.list && \
    apt-get update && \
    ACCEPT_EULA=Y apt-get install -y msodbcsql17 unixodbc-dev && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*
    
# Install ODBC libraries and OpenCV dependencies
RUN apt-get update && apt-get install -y \
    unixodbc \
    unixodbc-dev \
    libodbc1 \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Install tini only if not installed
RUN if ! command -v tini >/dev/null 2>&1; then \
        apt-get update && apt-get install -y tini && rm -rf /var/lib/apt/lists/*; \
    fi

WORKDIR /app/services/image_grouping

# --- copy requirements first for caching ---
COPY docker/requirements.txt ./requirements.txt

# Install deps with cache (only re-run if requirements.txt changes)
RUN pip install --no-cache-dir --upgrade -r requirements.txt

# --- now copy the rest of the code ---
COPY person_recognition/ ./person_recognition/
COPY image_face_detection/ ./image_face_detection/
COPY main.py ./main.py
COPY docker/docker-entrypoint.sh /usr/local/bin/
COPY docker/healthcheck.py ./docker/healthcheck.py
COPY config.py ./config.py
COPY .env ./.env

# Copy logger_config.py files to their respective folders
COPY person_recognition/logger_config.py ./person_recognition/logger_config.py
COPY image_face_detection/logger_config.py ./image_face_detection/logger_config.py

RUN chmod +x /usr/local/bin/docker-entrypoint.sh \
    && mkdir -p person_recognition/logs image_face_detection/logs

# Healthcheck
HEALTHCHECK --interval=30s --timeout=5s --retries=3 \
  CMD ["python", "docker/healthcheck.py"]

# Entrypoint
ENTRYPOINT ["tini", "--", "/usr/local/bin/docker-entrypoint.sh"]



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\docker\healthcheck.py ******************************************

import os
import sys

# This script performs a lightweight, non-intrusive healthcheck.
# It verifies the application is ready to run by checking for the presence of
# key files and directories, without connecting to the database or network.

def run_healthcheck():
    """
    Checks for the existence of required files and directories.
    Returns 0 for success, 1 for failure.
    """
    try:
        # Check for the main application entry point.
        if not os.path.exists("../main.py"):
            print("Healthcheck failed: main.py not found.")
            return 1
            
        # Check for the logs directory. This also verifies the host-mounted volume.
        if not os.path.isdir("../logs"):
            print("Healthcheck failed: logs directory not found.")
            return 1
            
        # All checks passed.
        print("Healthcheck passed: Application is ready.")
        return 0
        
    except Exception as e:
        print(f"Healthcheck failed due to an unexpected error: {e}")
        return 1

if __name__ == "__main__":
    exit_code = run_healthcheck()
    sys.exit(exit_code)



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\docker\docker-entrypoint.sh ******************************************

#!/bin/bash
set -e
set -x

# Load environment variables (DB credentials, etc.)
export DB_HOST=${DB_HOST:-sqlserver}
export DB_PORT=${DB_PORT:-1433}
export DB_USER=${DB_USER:-sa}
export DB_PASSWORD=${DB_PASSWORD:-YourStrong@Passw0rd}
export DB_NAME=${DB_NAME:-MediaDB}

SERVICE=${SERVICE:-detection}
MODE=${MODE:-loop}
INTERVAL_SECONDS=${INTERVAL_SECONDS:-900}
RECOGNIZER_ARGS=${RECOGNIZER_ARGS:--recluster}

mkdir -p person_recognition/logs image_face_detection/logs

run_service() {
    case $SERVICE in
        recognition)
            LOGFILE=person_recognition/logs/run.log
            echo "[$(date)] Running $SERVICE with args: $RECOGNIZER_ARGS" >> "$LOGFILE"
            python main.py $RECOGNIZER_ARGS
            ;;
        detection)
            LOGFILE=image_face_detection/logs/run.log
            echo "[$(date)] Running $SERVICE with args: $RECOGNIZER_ARGS" >> "$LOGFILE"
            python main.py $RECOGNIZER_ARGS
            ;;
        *)
            echo "Unknown SERVICE: $SERVICE" >&2
            exit 1
            ;;
    esac
}

if [ "$MODE" = "loop" ]; then
    while true; do
        run_service
        echo "[$(date)] Sleeping for $INTERVAL_SECONDS seconds..."
        sleep $INTERVAL_SECONDS
    done
else
    run_service
fi



#****************************************** C:\Users\ADMIN\deepframe-backend\services\image_grouping\docker\.env.example ******************************************

# .env.example

SERVICE=recognition
MODE=oneshot
INTERVAL_SECONDS=900
RECOGNIZER_ARGS=--recluster


