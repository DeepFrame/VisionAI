{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G9HMN0ZlWXo"
   },
   "source": [
    "# Face Detection Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUONO79zozo3"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-frjx58Oo3OM",
    "outputId": "7d212299-e136-4c98-8873-87af2d8116b4"
   },
   "outputs": [],
   "source": [
    "!pip install retina-face onnxruntime insightface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4Mo-3Ehl3Mh"
   },
   "source": [
    "- RetinaFace based detection with implementation of threshold (Confidence Score + Laplacian Variance for blurry images)\n",
    "- Cropping (112x112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ox495I30KFjQ",
    "outputId": "b907765c-f1fb-46db-d101-b2245a380678"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "import os\n",
    "\n",
    "def is_blurry(image, threshold=100.0):\n",
    "    \"\"\"Check if image is blurry using variance of Laplacian.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return fm < threshold, fm\n",
    "\n",
    "def make_square_with_padding(img, target_size=None, pad_color=(0,0,0)):\n",
    "    \"\"\"Pad image to square shape, optionally resize.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    size = max(h, w)\n",
    "    square_img = np.full((size, size, 3), pad_color, dtype=np.uint8)\n",
    "    y_offset = (size - h) // 2\n",
    "    x_offset = (size - w) // 2\n",
    "    square_img[y_offset:y_offset+h, x_offset:x_offset+w] = img\n",
    "    if target_size:\n",
    "        return cv2.resize(square_img, target_size, interpolation=cv2.INTER_AREA)\n",
    "    return square_img\n",
    "\n",
    "def process_image_bbox(image_path, output_dir,\n",
    "                       min_score=0.99, blur_threshold=100.0,\n",
    "                       margin_ratio=0.2, target_size=(112, 112)):\n",
    "    \"\"\"Detect faces, skip low-score & blurry ones, keep pixels intact.\"\"\"\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not load image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    faces = RetinaFace.detect_faces(image_path)\n",
    "    if not faces:\n",
    "        print(f\"No faces detected in {image_path}.\")\n",
    "        return\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    for i, key in enumerate(faces.keys()):\n",
    "        face = faces[key]\n",
    "        score = face.get(\"score\", 0.0)\n",
    "\n",
    "        if score < min_score:\n",
    "            print(f\"Face {i+1} skipped (low score={score:.4f})\")\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = face[\"facial_area\"]\n",
    "\n",
    "        bw, bh = x2 - x1, y2 - y1\n",
    "        margin_x = int(bw * margin_ratio)\n",
    "        margin_y = int(bh * margin_ratio)\n",
    "        x1 = max(0, x1 - margin_x)\n",
    "        y1 = max(0, y1 - margin_y)\n",
    "        x2 = min(w, x2 + margin_x)\n",
    "        y2 = min(h, y2 + margin_y)\n",
    "\n",
    "        crop_w, crop_h = x2 - x1, y2 - y1\n",
    "        if crop_w > crop_h:\n",
    "            diff = crop_w - crop_h\n",
    "            expand_top = diff // 2\n",
    "            expand_bottom = diff - expand_top\n",
    "            if y1 - expand_top >= 0 and y2 + expand_bottom <= h:\n",
    "                y1 -= expand_top\n",
    "                y2 += expand_bottom\n",
    "            else:\n",
    "                x1 += diff // 2\n",
    "                x2 -= diff - diff // 2\n",
    "        elif crop_h > crop_w:\n",
    "            diff = crop_h - crop_w\n",
    "            expand_left = diff // 2\n",
    "            expand_right = diff - expand_left\n",
    "            if x1 - expand_left >= 0 and x2 + expand_right <= w:\n",
    "                x1 -= expand_left\n",
    "                x2 += expand_right\n",
    "            else:\n",
    "                y1 += diff // 2\n",
    "                y2 -= diff - diff // 2\n",
    "\n",
    "        x1, x2 = max(0, x1), min(w, x2)\n",
    "        y1, y2 = max(0, y1), min(h, y2)\n",
    "\n",
    "        cropped_face = img[y1:y2, x1:x2]\n",
    "\n",
    "        blurry, fm = is_blurry(cropped_face, blur_threshold)\n",
    "        if blurry:\n",
    "            print(f\"Face {i+1} skipped (blurry, LapVar={fm:.2f})\")\n",
    "            continue\n",
    "\n",
    "        final_face = make_square_with_padding(cropped_face, target_size=target_size)\n",
    "\n",
    "        cv2_imshow(final_face)\n",
    "\n",
    "        save_path = os.path.join(\n",
    "            output_dir,\n",
    "            f'{os.path.splitext(os.path.basename(image_path))[0]}_TN_{i+1}.jpg'\n",
    "        )\n",
    "        cv2.imwrite(save_path, final_face)\n",
    "        print(f\"Saved face to: {save_path} (score={score:.4f}, LapVar={fm:.2f})\\n\")\n",
    "\n",
    "def main(images_dir='/content/images', output_dir='/content/detected_faces'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "\n",
    "    print(f\"Total number of images found for detection : {len(os.listdir(images_dir))}\")\n",
    "    for filename in os.listdir(images_dir):\n",
    "        if filename.lower().endswith(valid_extensions):\n",
    "            image_path = os.path.join(images_dir, filename)\n",
    "            process_image_bbox(image_path, output_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2Np4V2r3tov",
    "outputId": "d8e82a04-21ad-4688-d9ba-aadb2243aa8c"
   },
   "outputs": [],
   "source": [
    "print(f\"Finished processing. From {len(os.listdir('/content/images'))} images, the face detection system detected {len(os.listdir('/content/detected_faces'))} faces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6KJSyUemaet"
   },
   "source": [
    "# Face Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPM-CSXHota6"
   },
   "source": [
    "### Install dependencies first if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsWfhCc2owOv",
    "outputId": "677f055e-b853-4c83-c23b-3a78956f932d"
   },
   "outputs": [],
   "source": [
    "!pip install keras-facenet umap-learn scikit-learn matplotlib opencv-python tensorflow numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGIxALE3mz5J"
   },
   "source": [
    "### FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZF_Ga1Hnb7H",
    "outputId": "cccefda0-ed9e-4ade-dba3-0c3faf1a48b2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_facenet import FaceNet\n",
    "from sklearn.cluster import DBSCAN\n",
    "import umap\n",
    "import cv2\n",
    "\n",
    "# Load images from a folder\n",
    "image_folder = \"/content/detected_faces\"\n",
    "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder)\n",
    "               if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "images = []\n",
    "\n",
    "# Images Preprocessing (160x160 for FaceNet)\n",
    "for path in image_paths:\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (160, 160))\n",
    "    images.append(img)\n",
    "images = np.array(images)\n",
    "\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "\n",
    "# Extract embeddings\n",
    "embedder = FaceNet()\n",
    "embeddings = embedder.embeddings(images)\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIje1i3ooVyr"
   },
   "source": [
    "### Optional Dimentionality Reduction\n",
    "based on model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eDwfHuuoafu",
    "outputId": "fab02241-bcd9-4a5c-ceec-d21cef70e1b0"
   },
   "outputs": [],
   "source": [
    "# Dimensionality reduction (UMAP)\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.0, metric='cosine', random_state=42)\n",
    "embeddings_2d = reducer.fit_transform(embeddings)\n",
    "print(\"Reduced embeddings shape:\", embeddings_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzNRmA1vnJpH"
   },
   "source": [
    "# Face Clustering Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9O1zHYMnsTJ"
   },
   "source": [
    "### DBSCAN (eps=0.35, min_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yENzMWlPhpzP",
    "outputId": "2d5ef13b-f11b-402f-a48e-3fe96e761793"
   },
   "outputs": [],
   "source": [
    "# Clustering (DBSCAN)\n",
    "dbscan_eps = 0.35\n",
    "dbscan_min_samples = 3\n",
    "\n",
    "clustering = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples, metric=\"cosine\")\n",
    "labels = clustering.fit_predict(embeddings)\n",
    "print(\"Initial clusters:\", set(labels))\n",
    "\n",
    "num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(f\"Found {num_clusters} clusters (DBSCAN).\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1], c=labels, cmap='tab20')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "plt.title(\"Face Clusters (UMAP projection)\")\n",
    "plt.show()\n",
    "\n",
    "# Display faces by cluster\n",
    "import math\n",
    "\n",
    "def show_cluster_faces(images, labels):\n",
    "    unique_labels = set(labels)\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            print(\"Displaying noise/outliers:\")\n",
    "        else:\n",
    "            print(f\"Displaying cluster {label}:\")\n",
    "        cluster_indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        cluster_images = [images[i] for i in cluster_indices]\n",
    "\n",
    "        cols = 5\n",
    "        rows = math.ceil(len(cluster_images) / cols)\n",
    "        plt.figure(figsize=(15, 3*rows))\n",
    "        for i, img in enumerate(cluster_images):\n",
    "            plt.subplot(rows, cols, i+1)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "show_cluster_faces(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PavCJPPKnMWa"
   },
   "source": [
    "### DBSCAN (eps=0.25, min_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ss-3zHj2iOJ2",
    "outputId": "792ee015-fdce-4f2a-f852-987a0b9d14e2"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Clustering (DBSCAN)\n",
    "# -------------------------------\n",
    "clustering = DBSCAN(eps=0.25, min_samples=2, metric='cosine')\n",
    "labels = clustering.fit_predict(embeddings)\n",
    "\n",
    "num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(f\"Found {num_clusters} clusters (DBSCAN).\")\n",
    "\n",
    "# -------------------------------\n",
    "# Visualization\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1], c=labels, cmap='tab20')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "plt.title(\"Face Clusters (UMAP projection)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Display faces by cluster\n",
    "# -------------------------------\n",
    "import math\n",
    "\n",
    "def show_cluster_faces(images, labels):\n",
    "    unique_labels = set(labels)\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            print(\"Displaying noise/outliers:\")\n",
    "        else:\n",
    "            print(f\"Displaying cluster {label}:\")\n",
    "        cluster_indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        cluster_images = [images[i] for i in cluster_indices]\n",
    "\n",
    "        cols = 5\n",
    "        rows = math.ceil(len(cluster_images) / cols)\n",
    "        plt.figure(figsize=(15, 3*rows))\n",
    "        for i, img in enumerate(cluster_images):\n",
    "            plt.subplot(rows, cols, i+1)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "show_cluster_faces(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imcRonTlYVlK"
   },
   "source": [
    "# Re-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pf-4iGXM-ifX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from retinaface import RetinaFace\n",
    "from keras_facenet import FaceNet\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhR19Blx-poB"
   },
   "outputs": [],
   "source": [
    "def check_new_images(images_dir, faces_dir):\n",
    "    \"\"\"Compare processed images with raw images and return new ones.\"\"\"\n",
    "    processed = set([f.split(\"_TN_\")[0] for f in os.listdir(faces_dir)])\n",
    "    originals = set([os.path.splitext(f)[0] for f in os.listdir(images_dir)\n",
    "                     if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
    "    new_images = [f for f in originals if f not in processed]\n",
    "    return [os.path.join(images_dir, f) for f in os.listdir(images_dir)\n",
    "            if os.path.splitext(f)[0] in new_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdlLt1jN-szn"
   },
   "outputs": [],
   "source": [
    "#  Embeddings\n",
    "def get_embeddings(face_paths, embedder):\n",
    "    faces = []\n",
    "    for path in face_paths:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (160,160))\n",
    "        faces.append(img)\n",
    "    if len(faces) == 0:\n",
    "        return np.array([])\n",
    "    return embedder.embeddings(np.array(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txvql8R7-wcH"
   },
   "outputs": [],
   "source": [
    "#  Reclustering\n",
    "def recluster_embeddings(embeddings, labels, similarity_threshold=0.8,\n",
    "                         eps=0.35, min_samples=3):\n",
    "    \"\"\"Assign unlabeled faces (-1) to closest cluster or new DBSCAN cluster\"\"\"\n",
    "    labeled_indices = np.where(labels != -1)[0]\n",
    "    unlabeled_indices = np.where(labels == -1)[0]\n",
    "\n",
    "    if len(unlabeled_indices) == 0:\n",
    "        return labels\n",
    "\n",
    "    labeled_embeddings = embeddings[labeled_indices]\n",
    "    labeled_labels = labels[labeled_indices]\n",
    "    unlabeled_embeddings = embeddings[unlabeled_indices]\n",
    "\n",
    "    sim_matrix = cosine_similarity(unlabeled_embeddings, labeled_embeddings)\n",
    "    best_match_idx = np.argmax(sim_matrix, axis=1)\n",
    "    best_match_sim = np.max(sim_matrix, axis=1)\n",
    "\n",
    "    new_labels = labels.copy()\n",
    "    for i, (idx, sim) in enumerate(zip(unlabeled_indices, best_match_sim)):\n",
    "        if sim > similarity_threshold:\n",
    "            new_labels[idx] = labeled_labels[best_match_idx[i]]\n",
    "\n",
    "    remaining_unlabeled = np.where(new_labels == -1)[0]\n",
    "    if len(remaining_unlabeled) > 0:\n",
    "        reclust = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\")\n",
    "        reclust_labels = reclust.fit_predict(embeddings[remaining_unlabeled])\n",
    "\n",
    "        max_label = np.max(labels) if len(labeled_indices) > 0 else -1\n",
    "        reclust_labels[reclust_labels != -1] += max_label + 1\n",
    "\n",
    "        for i, idx in enumerate(remaining_unlabeled):\n",
    "            new_labels[idx] = reclust_labels[i]\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1FP-pfv-0ff"
   },
   "outputs": [],
   "source": [
    "#  Full Pipeline\n",
    "def process_pipeline(images_dir=\"/content/images\",\n",
    "                     faces_dir=\"/content/detected_faces\"):\n",
    "    os.makedirs(faces_dir, exist_ok=True)\n",
    "\n",
    "    embedder = FaceNet()\n",
    "\n",
    "    new_images = check_new_images(images_dir, faces_dir)\n",
    "    print(f\"ðŸ“¸ Found {len(new_images)} new images to process\")\n",
    "    new_faces = []\n",
    "\n",
    "    new_faces = []\n",
    "    for img_path in new_images:\n",
    "        new_faces.extend(process_image_bbox(img_path, faces_dir))\n",
    "\n",
    "    face_files = [os.path.join(faces_dir,f) for f in os.listdir(faces_dir)\n",
    "                  if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "    embeddings = get_embeddings(face_files, embedder)\n",
    "\n",
    "    if embeddings.size == 0:\n",
    "        print(\"âš ï¸ No faces found, skipping clustering\")\n",
    "        return [], []\n",
    "\n",
    "    labels_path = os.path.join(faces_dir,\"labels.npy\")\n",
    "    if not os.path.exists(labels_path):\n",
    "        clustering = DBSCAN(eps=0.35, min_samples=3, metric=\"cosine\")\n",
    "        labels = clustering.fit_predict(embeddings)\n",
    "    else:\n",
    "        old_labels = np.load(labels_path)\n",
    "        if len(old_labels) != len(face_files):\n",
    "            diff = len(face_files) - len(old_labels)\n",
    "            labels = np.concatenate([old_labels, -1*np.ones(diff, dtype=int)])\n",
    "        else:\n",
    "            labels = old_labels\n",
    "        labels = recluster_embeddings(embeddings, labels)\n",
    "\n",
    "    np.save(labels_path, labels)\n",
    "    print(f\"âœ… Processed {len(face_files)} faces into {len(set(labels)) - (1 if -1 in labels else 0)} clusters.\")\n",
    "\n",
    "    return labels, face_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJlR5OTH-6Hm"
   },
   "outputs": [],
   "source": [
    "#  Visualization\n",
    "def show_cluster_faces(images, labels, max_images=20):\n",
    "    unique_labels = sorted(set(labels))\n",
    "\n",
    "    for label in unique_labels:\n",
    "        cluster_indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        cluster_images = [images[i] for i in cluster_indices]\n",
    "\n",
    "        if label == -1:\n",
    "            print(f\"Displaying noise/outliers: {len(cluster_images)} images\")\n",
    "        else:\n",
    "            print(f\"Displaying cluster {label}: {len(cluster_images)} images\")\n",
    "            if len(cluster_images) > max_images:\n",
    "                print(f\"  (Showing first {max_images} of {len(cluster_images)} images)\")\n",
    "                cluster_images = cluster_images[:max_images]\n",
    "\n",
    "        cols = 5\n",
    "        rows = math.ceil(len(cluster_images) / cols)\n",
    "        plt.figure(figsize=(15, 3*rows))\n",
    "        for i, img in enumerate(cluster_images):\n",
    "            plt.subplot(rows, cols, i+1)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            if label == -1:\n",
    "                plt.title(f\"Noise #{cluster_indices[i]}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OW1mDrMP--jH",
    "outputId": "724a0bf2-09c0-4699-f048-e47565f7942a"
   },
   "outputs": [],
   "source": [
    "print(f'Images in images folder: {len(os.listdir(\"/content/images\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXriVzzCEEQn"
   },
   "outputs": [],
   "source": [
    "def process_image_bbox(image_path, output_dir,\n",
    "                       min_score=0.99, blur_threshold=100.0,\n",
    "                       margin_ratio=0.2, target_size=(112, 112)):\n",
    "    \"\"\"Detect faces, skip low-score & blurry ones, keep pixels intact.\"\"\"\n",
    "    saved_paths = []\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not load image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    faces = RetinaFace.detect_faces(image_path)\n",
    "    if not faces:\n",
    "        print(f\"No faces detected in {image_path}.\")\n",
    "        return\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    for i, key in enumerate(faces.keys()):\n",
    "        face = faces[key]\n",
    "        score = face.get(\"score\", 0.0)\n",
    "\n",
    "        if score < min_score:\n",
    "            print(f\"Face {i+1} skipped (low score={score:.4f})\")\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = face[\"facial_area\"]\n",
    "\n",
    "        bw, bh = x2 - x1, y2 - y1\n",
    "        margin_x = int(bw * margin_ratio)\n",
    "        margin_y = int(bh * margin_ratio)\n",
    "        x1 = max(0, x1 - margin_x)\n",
    "        y1 = max(0, y1 - margin_y)\n",
    "        x2 = min(w, x2 + margin_x)\n",
    "        y2 = min(h, y2 + margin_y)\n",
    "\n",
    "        crop_w, crop_h = x2 - x1, y2 - y1\n",
    "        if crop_w > crop_h:\n",
    "            diff = crop_w - crop_h\n",
    "            expand_top = diff // 2\n",
    "            expand_bottom = diff - expand_top\n",
    "            if y1 - expand_top >= 0 and y2 + expand_bottom <= h:\n",
    "                y1 -= expand_top\n",
    "                y2 += expand_bottom\n",
    "            else:\n",
    "                x1 += diff // 2\n",
    "                x2 -= diff - diff // 2\n",
    "        elif crop_h > crop_w:\n",
    "            diff = crop_h - crop_w\n",
    "            expand_left = diff // 2\n",
    "            expand_right = diff - expand_left\n",
    "            if x1 - expand_left >= 0 and x2 + expand_right <= w:\n",
    "                x1 -= expand_left\n",
    "                x2 += expand_right\n",
    "            else:\n",
    "                y1 += diff // 2\n",
    "                y2 -= diff - diff // 2\n",
    "\n",
    "        x1, x2 = max(0, x1), min(w, x2)\n",
    "        y1, y2 = max(0, y1), min(h, y2)\n",
    "\n",
    "        cropped_face = img[y1:y2, x1:x2]\n",
    "\n",
    "        blurry, fm = is_blurry(cropped_face, blur_threshold)\n",
    "        if blurry:\n",
    "            print(f\"Face {i+1} skipped (blurry, LapVar={fm:.2f})\")\n",
    "            continue\n",
    "\n",
    "        final_face = make_square_with_padding(cropped_face, target_size=target_size)\n",
    "\n",
    "        cv2_imshow(final_face)\n",
    "\n",
    "        save_path = os.path.join(\n",
    "            output_dir,\n",
    "            f'{os.path.splitext(os.path.basename(image_path))[0]}_TN_{i+1}.jpg'\n",
    "        )\n",
    "        cv2.imwrite(save_path, final_face)\n",
    "        print(f\"Saved face to: {save_path} (score={score:.4f}, LapVar={fm:.2f})\\n\")\n",
    "        saved_paths.append(save_path)\n",
    "\n",
    "    return saved_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "2bsKLqYI_BAo",
    "outputId": "365eade7-7da1-4a12-e8a3-9ad574c201ec"
   },
   "outputs": [],
   "source": [
    "labels, face_files = process_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "2vpBU8WyE0Z3",
    "outputId": "3583f844-d154-4f3f-8de5-87c344c71967"
   },
   "outputs": [],
   "source": [
    "labels, face_files = process_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPCN4PeT_C3e",
    "outputId": "44f4c69b-0354-48ac-bb03-83496c4c73a2"
   },
   "outputs": [],
   "source": [
    "# Final cluster information\n",
    "final_clusters = set(labels) - {-1}\n",
    "print(f\"Final clusters: {len(final_clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NzjGI9l4_Ew1",
    "outputId": "442a292f-9a28-4daa-b6ce-80eef72bb103"
   },
   "outputs": [],
   "source": [
    "# Dimensionality reduction for visualization\n",
    "reducer = umap.UMAP(n_components=2, metric='cosine', random_state=42)\n",
    "embeddings = get_embeddings(face_files, FaceNet())\n",
    "embeddings_2d = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WNYb-GaN93en",
    "outputId": "203b8132-ad7f-4d1e-a79a-0637d740ffbb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='tab20', s=10)\n",
    "plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "plt.title(\"Final Face Clusters (UMAP projection)\")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "\n",
    "unique_labels = sorted(set(labels))\n",
    "for label in unique_labels:\n",
    "    if label == -1:\n",
    "        plt.scatter([], [], c='gray', label=f'Noise ({-1})')\n",
    "    else:\n",
    "        plt.scatter([], [], c=scatter.cmap(scatter.norm(label)), label=f'Cluster {label}')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_miYnTxAxTAT"
   },
   "source": [
    "### Visualize reclustered results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_DaOP8hx_LG2",
    "outputId": "3442e8d1-38c2-46a3-c164-553e1543ac63"
   },
   "outputs": [],
   "source": [
    "# Display faces by cluster\n",
    "face_imgs = [cv2.cvtColor(cv2.imread(f), cv2.COLOR_BGR2RGB) for f in face_files]\n",
    "show_cluster_faces(face_imgs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Kj8AjIB-blv"
   },
   "source": [
    "# Move Images of Individual in one folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3kahBl3Fg5e"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_faces_by_cluster(face_files, labels, faces_dir):\n",
    "    unique_labels = set(labels)\n",
    "    for lbl in unique_labels:\n",
    "        cluster_name = f\"person_{lbl}\" if lbl != -1 else \"outliers\"\n",
    "        cluster_dir = os.path.join(faces_dir, cluster_name)\n",
    "        os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "        indices = [i for i, l in enumerate(labels) if l == lbl]\n",
    "        for i in indices:\n",
    "            src = face_files[i]\n",
    "            dst = os.path.join(cluster_dir, os.path.basename(src))\n",
    "\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5IKc5H_FllG"
   },
   "outputs": [],
   "source": [
    "save_faces_by_cluster(face_files, labels, '/content/individuals_recognized')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
